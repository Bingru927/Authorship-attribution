{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11618168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json \n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 导入所需的库\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(data_dir):\n",
    "    dirs = os.listdir(data_dir)\n",
    "    \n",
    "    for path in dirs:\n",
    "        if \"_train_\" in path:\n",
    "            train_path = os.path.join(data_dir,path)\n",
    "        elif \"_val_\" in path:\n",
    "            val_path = os.path.join(data_dir,path)\n",
    "        elif \"_test_\" in path:\n",
    "            test_path = os.path.join(data_dir,path)\n",
    "    return train_path, val_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "def preprocess(text):\n",
    "    POS = []\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove website link\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'@$%^&*()\\\\', '', text)  #Remove illegal characters\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\n+', 'newline', text)\n",
    "    text = re.sub(r\"[:;=][)D]|[(][=:;]\", 'emoji', text)\n",
    "    t = nltk.word_tokenize(text)\n",
    "    t = nltk.pos_tag(t)\n",
    "    tag = []\n",
    "    for i in t:\n",
    "        tag.append(nltk.tag.util.tuple2str(i))\n",
    "    text = \" \".join(tag)\n",
    "    POS.append(text)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataJSON(route):\n",
    "    with open(route,\"r\",encoding=\"utf-8\") as f:\n",
    "        result = [json.loads(line) for line in f.read().splitlines()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, ngram_range=(1,2)):\n",
    "    data = pd.DataFrame(getDataJSON(path))\n",
    "    data['process'] = data['comment'].str.lower().apply(preprocess)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'darkreddit_authorship_attribution_anon'\n",
    "\n",
    "train_path, val_path, test_path = get_data_path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff6b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_data(train_path)\n",
    "val = get_data(val_path)\n",
    "test = get_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.concat([train, val, test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['comment'][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function for converting text to vectors\n",
    "def vectorize_text(text_data):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    vectorized = vectorizer.fit_transform(text_data)\n",
    "    return vectorized, vectorizer\n",
    "# def vectorize_text(text_data):\n",
    "#     vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=10000,stop_words='english')\n",
    "#     vectorized = vectorizer.fit_transform(text_data)\n",
    "#     tfidf_transformer = TfidfTransformer()\n",
    "#     vectorized = tfidf_transformer.fit_transform(vectorized)\n",
    "#     return vectorized, tfidf_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294bd006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to run LDA and output clustering results\n",
    "def run_lda(text_data, n_topics):\n",
    "    vectorized, vectorizer = vectorize_text(text_data)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics)\n",
    "    lda.fit_transform(vectorized)\n",
    "    topic_words = vectorizer.get_feature_names_out()\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:9]\n",
    "        topic_keywords.append([topic_words[i] for i in top_keyword_locs])\n",
    "    doc_topics = lda.transform(vectorized)\n",
    "    clusters = np.argmax(doc_topics, axis=1)\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eda962",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clusters'] = run_lda(data['comment'],9)\n",
    "print(data['clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134be141",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = data[data['clusters']==0]\n",
    "df_1 = data[data['clusters']==1]\n",
    "df_2 = data[data['clusters']==2]\n",
    "df_3 = data[data['clusters']==3]\n",
    "df_4 = data[data['clusters']==4]\n",
    "df_5 = data[data['clusters']==5]\n",
    "df_6 = data[data['clusters']==6]\n",
    "df_7 = data[data['clusters']==7]\n",
    "df_8 = data[data['clusters']==8]\n",
    "# df_9 = data[data['clusters']==9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a939d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax0 = sns.countplot(x=\"author\", data=df_0)\n",
    "plt.title(\"Author Distribution of Cluster 1 \",fontsize=16)\n",
    "plt.xlabel(\"Author\",fontsize=16)\n",
    "plt.ylabel(\"Count\",fontsize=16)\n",
    "plt.savefig('1.png', dpi=300, bbox_inches='tight')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(x=\"author\", data=df_1)\n",
    "plt.title(\"Author Distribution of Cluster 2 \",fontsize=16)\n",
    "plt.xlabel(\"Author\",fontsize=16)\n",
    "plt.ylabel(\"Count\",fontsize=16)\n",
    "plt.savefig('2.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(x=\"author\", data=df_2)\n",
    "plt.title(\"Author Distribution of Cluster 3 \")\n",
    "plt.xlabel(\"Author\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(x=\"author\", data=df_3)\n",
    "plt.title(\"Author Distribution of Cluster 4 \",fontsize=16)\n",
    "plt.xlabel(\"Author\")\n",
    "plt.ylabel(\"Count\",fontsize=16)\n",
    "plt.savefig('4.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(x=\"author\", data=df_4)\n",
    "plt.title(\"Author Distribution of Cluster 5 \",fontsize=16)\n",
    "plt.xlabel(\"Author\",fontsize=16)\n",
    "plt.ylabel(\"Count\",fontsize=16)\n",
    "\n",
    "plt.savefig('5.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(x=\"author\", data=df_5)\n",
    "plt.title(\"Author Distribution of Cluster 6 \")\n",
    "plt.xlabel(\"Author\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(x=\"author\", data=df_6)\n",
    "plt.title(\"Author Distribution of Cluster 7 \")\n",
    "plt.xlabel(\"Author\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(x=\"author\", data=df_7)\n",
    "plt.title(\"Author Distribution of Cluster 8 \",fontsize=16)\n",
    "plt.xlabel(\"Author\",fontsize=16)\n",
    "plt.ylabel(\"Count\",fontsize=16)\n",
    "plt.savefig('8.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(x=\"author\", data=df_8)\n",
    "plt.title(\"Author Distribution of Cluster 9\",fontsize=16)\n",
    "plt.xlabel(\"Author\",fontsize=16)\n",
    "plt.ylabel(\"Count\",fontsize=16)\n",
    "plt.savefig('9.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec8a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_test(df):\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "    X = count_vectorizer.fit_transform(df['comment'])\n",
    "    # y = count_vectorizer.fit_transform(df['author'])\n",
    "\n",
    "    print(\"CountVectorizer done\")\n",
    "\n",
    "    # Convert word frequency vectors to tf-idf vectors using TfidfTransformer\n",
    "    print(\"Start TfidfTransformer\")\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X = tfidf_transformer.fit_transform(X)\n",
    "    # y = tfidf_transformer.transform(y)\n",
    "    print(\"TfidfTransformer done\")\n",
    "    # vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "    # X = vectorizer.fit_transform(df['comment'])\n",
    "    y = np.asarray(df['author'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=0.2, # keep 20% for testing\n",
    "                                                        random_state=2 # pass an int for reproducible rtesult\n",
    "                                                        )\n",
    "\n",
    "    model = MLPClassifier(max_iter=100000, solver='adam', learning_rate='invscaling', hidden_layer_sizes=(172,),\n",
    "                      alpha=1e-05, activation='logistic')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 测试\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred,zero_division=0))\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3eb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# , df_3,df_4,df_5,df_6,df_5,df_6,df_7,df_8,df_9,df_4\n",
    "df_list =[df_0, df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602bae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "for df in df_list:\n",
    "    acc = train_test(df)\n",
    "    accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(accs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0755c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_all = train_test(data)\n",
    "print(acc_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bad017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08919c9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
